---
title: "Modelling Property Prices"
author: "Dilshan Wijesena"
date: "01/31/2021"
output: 
  bookdown::html_document2:
    toc: yes
    toc_depth: '3'
    number_sections: yes
header-includes:
 \usepackage{float}
 \floatplacement{figure}{H}
---


```{r set up, message = FALSE, warning = FALSE, error = FALSE}

# load libraries
library(nano)
library(data.table)
library(skmir)
library(plotly)
library(h2o)
library(kableExtra)
library(bookdown)

# response variable
response <- "sale_price"

# directory to save final nano object
path <- "C:/Users/dilsh/Documents/R Packages/nano/vignettes/Nano objects"

```



\newpage

# Introduction
The purpose of this article is to illustrate the functionality of the `nano` package. As an illustrative example, we shall build a machine learning model to predict Sydney property prices using the `property_prices_unclean` dataset from the `nano` package. This dataset is based on the `SydneyRealEstate` dataset provided by Ruppert and Wand (2018). Entries in the original clean dataset have been randomly removed or replaced with NAs and outliers to simulate an unclean dataset.  

The first section of the report demonstrates the functions available to automate the data preparation and visualisation process. Then the power of the `nano` objects will be shown in how to easily create a wide variety of machine learning models and compare them. Lastly, a wide variety of techniques will be implemented to help understand the model's prediction mechanism and evaluate its performance on an unseen dataset. 


# Data Preparation

## Read in Data and Perform Checks

Load unclean dataset and randomly subset 2,000 rows to reduce computational run time. The `skim` function from the `skimr` package provides a quick but detailed summary for each variable. It can be seen that the variables: sale_qtr, dist_to_rail_line, dist_to_rail_station, dist_to_sealed_road, dist_to_unsealed_road, NO2, dist_to_university and sale_price are missing values. As well, it appears dist_to_unsealed_road and air_noise have possible outliers with maximum values of 999,999. 

```{r read in data, message = FALSE, error = FALSE}

# load data
data("property_prices_unclean")
data <- property_prices_unclean
rm(property_prices_unclean)

# select 2,000 rows
set.seed(2020)
data <- data[sample(nrow(data), 2000)]

# check data
head(data[, 1:6], 5) # no issues
tail(data[, 1:6], 5) # no issues
skimr::skim_without_charts(data) # several variables have a few missing values and possible data entry errors

```


## General Data Cleaning 

The `tidy_data` function performs general cleaning of dataset. All character variables and variables with low number of unique levels (less than `thresh`) are converted to factor types as well. Further, it removes all entries with missing values in the response variable.

```{r data cleaning, error = FALSE}

# general cleaning of data
data <- nano::tidy_data(data         = data, 
                        thresh       = 6,
                        retain_names = FALSE,
                        response     = response)

```

As can be seen from the above output messages, 4 entries were removed due to missing values in the response variable, sale_price. Further sale_date, rtm where converted from character to factor type while sale_qtr was correctly converted from numeric to factor type. 


## Stepwise VIF Selection {#VIF}

Collinearity between variables is an important issue to identify before beginning the modelling process. The presence of collinearity introduces additional uncertainty in the model fitting process and result in higher variance models (Kuhn & Johnson 2013). Kuhn and Johnson (2013) proposes the use of the VIF measure to identify collinearity and remove variables with a VIF greater than a pre-defined threshold. Kuhn and Johnson (2013) notes this method does indeed result in significant improvement in model performance. 

The approach recommended above is implemented by the `vif_step` function. This function calculates the VIF for each variable and then removes the variable with the highest VIF if it is above a certain threshold. This process is then repeated until all variables have a VIF lower than the threshold. If there are certain variables which are known to be required for the modelling process, these can be ignored by using the `ignore` argument. 

Initially, the `remove` argument is set to `FALSE` to view the VIFs for each variable. This can reveal which variables are likely to removed by the process. If any of these variables are desired to be kept, the function can be run again with `remove = TRUE` specifying the desired variables in the `ignore` argument. Specifying `trace = TRUE` will show the VIFs for each variable after each iteration and will print out which variable is removed (if removed).


```{r vif step}

vif_init <- nano::vif_step(data   = data, 
                           ignore = c(response), 
                           thresh = 5, 
                           remove = FALSE,
                           trace  = TRUE)

data_vif <- nano::vif_step(data   = data, 
                           ignore = c(response, "crime_rate", "latitude", 
                                      "longitude", "dist_to_rail_station"), 
                           thresh = 5, 
                           remove = TRUE,
                           trace  = FALSE)
data <- data.table::copy(data_vif$data)

```


## Impute Data

The `cleanliness_smry` function returns a list containing a detailed summary on how clean the dataset is. It provides information on: NAs, blanks, outliers, duplicates and special character. Outliers are determined by calculating how many standard deviations each value is from the mean and if it is greater than `outlier_sd`. Patterns in missing values can be identified using the `missing_pattern` function. This can be used to identify whether missing values are random, non-informative or informative.  

Using the information produced from the `cleanliness_smry` function, the `impute` function can be used to impute missing values. There are two methods available for imputing values. Values can be simply imputed with their mean/mode (depending if numeric or factor type), or the MICE method can be implemented which calculates multivariable imputations by using chained equations. This method is derived from the `mice` package.  

Similarly to the `vif_step` function, variables which are not desired to be imputed can be specified by the `impute_ignore` argument. As well, outliers can be imputed using the `impute_outlier` argument. The argument can be set to `Inf` if outliers are not desired to be imputed.  


```{r missing summary, warnings = FALSE, error = FALSE, results = "hide"}

# missing summary
data_missing_smry <- nano::cleanliness_smry(data       = data, 
                                            ignore     = c(response),
                                            outlier_sd = 10)
```

```{r imputation, warning = FALSE, message = FALSE, error = FALSE}

data <- data.table::copy(data_missing_smry$unique_rows) # remove duplicates
data_missing_smry$missing_smry # summary of blanks and NAs

head(data_missing_smry$outlier_rows[, 1:6]) # only true outliers are in air_noise and dist_to_unsealed_road

# identify any missing patterns
data_missing_pat <- nano::missing_pattern(data   = data,
                                          ignore = c(response),
                                          plot   = FALSE)
data_missing_pat$plot() # no pattern to missing observations

# impute missing values and outliers
data_impute <- nano::impute(data           = data, 
                            method         = "mice",
                            impute_ignore  = c("crime_rate", "dist_to_park", 
                                               "dist_to_school", "dist_to_medical"),
                            pred_ignore    = c(response),
                            impute_outlier = 10)
# save final dataset
data_fin <- data.table::copy(data_impute$imputed_data)

```



## All in One and More!

All the above functions can be implemented by the single function `data_prep`. Further, this function automatically removes variables with low variables, balances and scales data, and provides splits into training, testing, holdout and folds for cross-validation. 

It is recommended to run the `data_prep` function rather than the separate functions. This is because the `data_prep` function applies each of the methods on the training dataset separately and then replicates the same method on the testing/holdout dataset. For example, imputation is initially based only on the training dataset. Then the same imputation method is applied on the testing/holdout dataset. This is to avoid data leakage.   


```{r data_prep, warning = FALSE, error = FALSE, cache = TRUE}

data("property_prices_unclean")
data_ex <- data.table::copy(property_prices_unclean)
rm(property_prices_unclean)

# select 2,000 rows
set.seed(2020)
data_ex <- data_ex[sample(nrow(data_ex), 2000)]

## all in one function
data_process <- nano::data_prep(data          = data_ex, 
                                response      = response,
                                split_or_fold = 5,
                                holdout_ratio = 0.1,
                                unique_row    = TRUE,
                                rm_low_var    = TRUE,
                                freq_thresh   = 95/5,
                                impute        = TRUE,
                                rm_outliers   = 10,
                                impute_method = "mice",
                                impute_ignore = c("dist_to_park", "dist_to_sealed_road", 
                                                  "dist_to_school", "sale_date"),
                                pred_ignore   = c(response),
                                vif_select    = TRUE,
                                vif_ignore    = unique(c(response, "crime_rate", "latitude", 
                                                         "longitude", "sale_qtr", "dist_to_rail_station")), 
                                vif_thresh    = 5,
                                thresh        = 6,
                                balance       = FALSE,
                                scale         = FALSE)

# Note: NAs still in dist_to_sealed_road since was ignored
# impute dist_to_sealed_road separately and set impute_outlier to Inf
data_ex_impute <- nano::impute(data           = data_process$data, 
                               method         = "mice",
                               pred_ignore    = c(response),
                               impute_outlier = Inf)
# missing summary
data_ex_missing_smry <- nano::cleanliness_smry(data       = data_ex_impute$imputed_data, 
                                               ignore     = c(response),
                                               outlier_sd = 10)
# no missing values, save final dataset
data <- data.table::copy(data_ex_impute$imputed_data)

# convert sale_date to date type and extract weekday from sale_date
data[, sale_date := as.Date(as.character(sale_date), "%d/%m/%Y")
     ][, wday := as.factor(as.character(lubridate::wday(sale_date, label = TRUE)))]


```


```{r echo = FALSE, warning = FALSE, message = FALSE, fig.height=3, fig.align = "center", fig.cap = "Distribution of sale_price in training and holdout dataset. Left: training dataset. Right: holdout dataset."}

# compare plots to confirm balanced data
# remove zero values and filter dataset to better see the histogram
p1 <- ggplot(data = data[data_id == "train"]) + 
  geom_histogram(mapping = aes(x = sale_price), fill = "royalblue") +
  lares::theme_lares2(bg_colour = "white") # histogram of training data 
p2 <- ggplot(data = data[data_id == "holdout"]) + 
  geom_histogram(mapping = aes(x = sale_price), fill = "royalblue") +
  lares::theme_lares2(bg_colour = "white") # histogram of test data
# display plots
gridExtra::grid.arrange(p1, p2, nrow = 1) # plots are very similar as expected

```




# Exploratory Data Analysis {#EDA} 

Brief data exploration was performed to develop an initial expectation of the relationship between the predictors and the response variable. These expectations are then later compared when assessing the final select model (section \@ref(check)) to provide a basic sense check on the reasonableness of the model's prediction mechanism.

All plots are produced using the `plotly` package. A key advantage of plots produced by `plotly` compared to other package such as `ggplot` is their interactive features which allows for more informative and easier to view plots. Further, the package offers numerous types of visualisation methods with great flexibility in functionality. 

The below plot shows the distribution of sale price in the training dataset. As expected, the distribution is strongly positive skewed, following perhaps a gamma or Tweedie distribution.  

```{r echo = FALSE, warning = FALSE, message=FALSE, fig.align="center"}

## histogram of sale_price
fig <- plot_ly(x = data$sale_price[data$data_id == "train"],
               type = "histogram",
               histnorm = "probability") %>% 
  layout(title = "Distribution of Sale Price",
         xaxis = list(title = response),
         yaxis = list(title = "Frequency",
                      hoverformat = ".2f"))
htmltools::div(fig, align = "center")
```

Next, the numeric variables are banded using the `band_data` function from the `nano` package. From the banded variables, a series of one-way plots are produced below. These plots will have help provide us an insight on the relationships between the variables and the response. However, it is important to take care when interpreting one-ways since they may be influenced by spurious or confounding variables, resulting in misleading relationships. 

* lot size - as lot size increases, there is a general increase in average sale price as expected.
* crime rate - initially, as crime rate increases, there is a decrease in sale price. However, suprisingly as crime rate further increases, the sale price increases. This may be due to confounding variables. 
* income - as income increases, sale price generally increases as expected. Note, the average sale price in the $[0,400)$ band is higher than the sale price in the $[400, 800)$ band. This may be due to retirees (hence $0$ or low income) or property investors (with low reported income) who are able to purchase more expensive properties. 
* distance to railway station - as distance to closest railway station increases, sale price initially decreases, then increases. This may reflect offsetting advantage of convenience by being close to a railway station versus the disadvantage of being situated in a busy, crowded region. From the graph, it appears the advantages only outweighs the disadvantages for properties within a 2 km radius of a railway station. 

```{r band data, warning = FALSE, message = FALSE}
## create bands
# intervals for bands
intervals <- list(lot_size             = seq(400, 2000, 200),
                  crime_rate           = seq(0.1, 0.6, 0.05),
                  income               = seq(0, 2000, 400),
                  dist_to_rail_station = seq(0, 10, 2),
                  foreigner_ratio      = seq(0, 1, 0.2),
                  NO                   = seq(0, 4, 1),
                  dist_to_university   = seq(0, 10, 2))
# band data
data_bnd <- data.table::copy(data)
nano::band_data(data             = data_bnd,
                intervals        = intervals,
                trunc_left       = TRUE,
                unmatched_bucket = "Other")

```


```{r echo = FALSE, warning = FALSE, message=FALSE, fig.height = 2.5, fig.width = 5}

# function summarise data by variable
one_way_smry <- function(data, var, response) {
  smry <- data[,
               .(mean = mean(get(response)), count = .N), 
               by = eval(paste0(var, "_bnd"))]
  return(smry)
}

# function to produce list of one-ways
one_way_plot <- function(data, vars, response) {
  
  # initalise list to hold all plots
  plots <- rep(list(NA), length(vars))
  
  # plot one-way for each variable
  for (var in vars) {
    x <- one_way_smry(data, var, response)

    # parameters for second axis
    rhs <- list(tickfont = list(color = "red"),
                overlaying = "y",
                side       = "right",
                title      = "Count",
                rangemode = "tozero",
                showgrid = FALSE
    )

    # initialise empty plot
    fig <- plotly::plot_ly() 
    # add first layer of plots - bar graph of mean sale_price
    fig <- fig %>% add_bars(x = x[[1]], y = x$mean, name = paste0("Average ", response),
                            yaxis = "y")
    # add second layer of plots - line graph of count (right y-axis)
    fig <- fig %>% add_lines(x = x[[1]], y = x$count, name = paste0("Count of ", var), 
                             yaxis = "y2")
    # set layout so bar graph uses left axis and line graph uses right axis
    fig <- fig %>% layout(
      title  = paste0(response, " vs ", var), 
      yaxis2 = rhs,
      xaxis  = list(title = var),
      yaxis  = list(title = response,
                    hoverformat = ",.2s"),
      showlegend = FALSE,
      margin = list(r = 40)
    )
    plots[[var]] <- fig
  }
  
  return(plots)
} 

one_ways <- one_way_plot(data     = data_bnd[data_id == "train"], 
                         vars     = names(intervals), 
                         response = "sale_price")

one_ways$lot_size # increasing as expected
one_ways$crime_rate # interestingly, higher sale prices in areas with higher crime rates. Maybe more crime (e.g. theft) due to more affluent area
one_ways$income # strong relationship. Increasing as expected
one_ways$dist_to_rail_station # decrease and then increase as expected
#one_ways$foreigner_ratio # skewed by single observation in [0, 0.2)
#one_ways$NO # interestingly, increases. Perhaps more expensive properties in busy city areas which have higher NO levels

```

The boxplot plot in figure 3.1 illustrates the distribution of sale price by each weekday. Interestingly, the least properties are bought during the weekend with Saturday having the lowest average sale price by far. Properties bought at the start or end of the week are sold for much higher prices. The plot on the right of figure 3.1 is a time series of the number of properties sold over the year 2001. The relatively straight line suggests there is no significant fluctuations in buyer's behavior during the year.  

```{r echo = FALSE, warning = FALSE, message = FALSE, fig.height = 4, fig.cap = "Left: Box plot of sale price by weekday. Right: Time series of number of properties sold."}

# fig1 <- plot_ly(data_bnd, 
#                 y = ~sale_price, 
#                 color = ~sale_qtr, 
#                 type = "box",
#                 boxpoints = "all", 
#                 jitter = 0.1,
#                 pointpos = -1.8)
# fig1
# 

fig1 <- plot_ly(data_bnd, 
                y = ~sale_price, 
                color = ~wday, 
                type = "box",
                boxpoints = "all", 
                jitter = 0.1,
                pointpos = -1.8) %>% 
  layout(showlegend = FALSE,
         yaxis = list(hoverformat = ",.2s"))

# sale_prices
# fig1 <- plot_ly(data_bnd,
#                 x = ~sale_date, 
#                 y = ~sale_price, 
#                 mode = 'markers',
#                 type = "scatter",
#                 text = paste("days from today"))
# fig1

# number of properties sold
date <- data.table::data.table(sale_date = data_bnd[order(sale_date), sale_date])
date[, ones := 1
     ][, sold_num := cumsum(ones)]
fig2 <- plot_ly(date,
                x = ~sale_date, 
                y = ~sold_num, 
                mode = 'markers',
                type = "scatter",
                name = "Properties sold") %>% 
  layout(showlegend = FALSE)

subplot(fig1, fig2, nrows = 1)

```


# Model Fitting 

## Nano Object

At the core of the `nano` package is the `nano` object. This is a S3 object which stores detailed information of all grids/models built. The `nano` object greatly simplifies and streamlines the modelling process as all grids and models are safely stored in the same location. This prevent cluttering of workspace and allows all previous information to be easily accessed. Further, it allows for powerful techniques to be used compare the model performance across different models. This is a feature which is not available in other packages (e.g. `DALEX` and `shapr`).

An empty `nano` object can be initialised by running the below code.

```{r first nano object, message = FALSE, warning = FALSE}

nano <- nano::create_nano()

```

Initially, a `nano` object holds 10 slots, one for each grid of models. When all the slots become populated, the number of slots will be automatically increased. Each slot contains 10 fields which are:

* grid - grid of models built. Populated using the `nano_automl` or `nano_grid` function.
* model - chosen model from corresponding grid (grid contained in the same slot). This field is populated by default with the best model in the grid. However, the model can be swapped with another model from the grid using the `switch_model` function. 
* metric - the training, test. cross-validation and holdout metrics (whichever are relevant) for the corresponding model. This field is populated by default. The values can be extracted/plot using the `nano_metric` function. 
* data - the dataset (stored as data.table) used to build the corresponding grid. This field is populated by default. 
* varimp - the variable importance of the corresponding model. Initially, when a grid is built, this field will remain empty. To calculate/plot the variable importance, use the `nano_varimp` function.  
* pdp -  the partial dependencies of the corresponding model. Initially, when a grid is built, this field will remain empty. To calculate/plot the partial dependencies, use the `nano_pdp` function. 
* ice - the initial conditional expectations of the corresponding model. Initially, when a grid is built, this field will remain empty. To calculate/plot the ICE, use the `nano_ice` function. 
* interaction - the pair-wise interaction value of the corresponding model. Initially, when a grid is built, this field will remain empty. To calculate/plot the ICE, use the `nano_interaction` function. 
* meta - contains useful meta information about its corresponding model such list of predictors, model type and hyper-parameters. This field is automatically populated when a grid is built.
* n_model - current number of grids/models in nano object. This field is automatically updated. 


## Model Evaluation

The 5-fold cross-validation technique was used to evaluate the models built since it is an attractive approach for selecting from various models, especially in cases where the degrees of freedom cannot be calculated, as in tree-based models (James, Witten, Hastie & Tibshirani 2013). 

For evaluating model performance, the residual deviance and MSE were the two main metrics used. Residual deviance provides information on the maximum likelihood and was chosen since it is a relatively robust metric against a variety of different distributions. MSE was chosen since it is sensitive to outliers which is desirable for predicting property prices since we do indeed wish to be able to accurately predict prices of high-end properties as well. For the remainder of the report, all metrics considered will be the cross-validation metrics (calculated by averaging the metric error across the holdout folds) unless specified. 


## H2O Cluster Connection 

There are two function which are able to be used to build models: `nano_automl` and `nano_grid`. `nano_automl` automatically builds a grid of various different ML models and finds the model with the best performance. `nano_grid` can be used to build a single model or for hyper-parameter tuning.

All the modelling, diagnostics and performance testing are based on the `H2O` package. `H2O` is a fast, scalable, open-source machine learning platform. A key advantage of `H2O` compared to other popular machine learning packages is that `H2O` algorithms are all multi-core enabled, allowing for parallel processing.  

Before modelling with the `nano` package, a connection to a H2O cluster must always be first initialised before building any models.  

```{r H2O connection, message = FALSE, warning = FALSE}

# start h2o connection to cluster
nano::nano_init()

```

## Building Nano Object

Currently, the `nano` object is empty and holds no information. We can build our first grid of models using `nano_automl`. This will populate the first slot of the `nano` object.

```{r echo = FALSE, warning = FALSE, message = FALSE}

# load previously made nano object
nano <- nano::nano_load(path)

```


```{r Initial AutoML model, warning = FALSE, eval = FALSE}

# create grid of autoML models to set initial benchmark
nano <- nano::nano_automl(nano             = nano, 
                          response         = response,
                          ignore_vars      = c("sale_date", "fold", "data_id"),
                          data             = data,
                          fold_column      = "fold",
                          train_test       = "data_id",
                          max_models       = 6,
                          grid_description = "Initial grid of models for benchmarking.")
```

```{r warning = FALSE}
# hyper-parameters of best model
unlist(nano$meta$meta_1$hyper_params)
unlist(nano$meta$meta_1$search_conditions)
nano$grid$grid_1@summary_table$metric_smry
```

The above summary tables shows the cross-validation metrics for each of the models built. As can be seen from the table, 4 GBMs, 1 random forest (distributed random forest) and 1 GLM were constructed. The GBM_3 model performs the best in both residual deviance and MSE.


## Selecting Distribution: Gamma vs Tweedie

From the initial grid of models built, it appears the GBM model provides the best fit to the data. Hence, for the remainder of the report, GBM models will be fitted to the data by tuning various parameters. 

In a GBM model, different distributions can be assumed for the loss function. Given the positive skewed distribution of property prices identified in section \@ref(EDA), the "gamma" and "Tweedie" distribution would be the two most appropriate distributions to use. The use of the Tweedie distribution introduces a further additional parameter, called "tweedie_power" ($p$), which take on values $1 < p < 2$. In fact, $p = 2$ corresponds to the gamma distribution.    

To determine which distribution to select, a saturated GBM model for each distribution was built using all variables available in the dataset to build the model. Enough trees were initially used (500 trees) to ensure the training error stabilised with early stopping implemented to control over-fitting. For the gamma distribution, a single saturated model was built while for the Tweedie distribution, a grid of four saturated models were built by tuning the tweedie_power. 


```{r tweedie_power tuning, warning = FALSE, eval = FALSE}

# set initial parameters - this will be tuned at the end 
hyper_params <- nano$meta$meta_1$hyper_params
# increase ntrees to ensure loss function stabilises or fitting terminates 
hyper_params$ntrees <- 500
# decrease maximum depth to reduce variance
hyper_params$max_depth <- 5 # rule of thumb: floor(sqrt(ncol(data)))
# reduce learning rate since large number of tree and help reduce variance
hyper_params$learn_rate <- 0.01

## Saturated tweedie distribution - tuning tweedie_power 
hyper_params$distribution <- "Tweedie"
hyper_params$tweedie_power <- seq(1.2, 1.8, 0.2)

# build grid for tweedie distribution
nano <- nano::nano_grid(nano                = nano,
                        response            = response,
                        algo                = "gbm", 
                        data                = data,
                        fold_column         = "fold",
                        hyper_params        = hyper_params,
                        strategy            = "Cartesian",
                        stopping_metric     = "deviance",
                        stopping_tolerance  = 0.01,
                        stopping_rounds     = 5,
                        score_tree_interval = 10,
                        grid_description    = "Saturated GBM Tweedie distribution tuning tweedie_power.")
```


```{r echo = FALSE, warning = FALSE, message=FALSE, fig.align="center", fig.height = 3, fig.width = 4, fig.cap = "Training deviance vs number of trees built."}

# check loss metrics vs trees
plot(nano$model$model_2, timestep = "AUTO", metric = "deviance") # enough trees are built, early stopping at ~ 350 trees. Keep to 500 trees.
```



```{r saturated gamma, warning = FALSE, eval = FALSE}

# Saturated gamma distribution
hyper_params$distribution <- "Gamma"
hyper_params$tweedie_power <- NULL

nano <- nano::nano_grid(nano                = nano,
                        response            = response,
                        algo                = "gbm", 
                        data                = data,
                        fold_column         = "fold",
                        hyper_params        = hyper_params,
                        strategy            = "Cartesian",
                        stopping_metric     = "deviance",
                        stopping_tolerance  = 0.01,
                        stopping_rounds     = 5,
                        score_tree_interval = 10,
                        grid_description    = "Saturated GBM gamma distribution.")
plot(nano$model$model_3, timestep = "AUTO", metric = "deviance")
```

```{r initial metrics}
# compare metrics
nano$grid$grid_2@summary_table$grid_smry # tweedie 1.8 saturated model performs the best across all metrics 

```

Firstly, from figure 4.1, it can be seen 500 trees is enough trees to ensure the loss error stabilises (in this case, early stopping terminates the training process). From the above summary table, models with higher tweedie_power performs better. Further this model performs better than the GBM with gamma distribution. 

The three model's prediction mechanism will also be compared to examine which model is most inline with our expectations. The `nano_pdp` function is used to calculate the PDPs of each model in the specified slots for the selected variables. The function also produce plots of the PDPs providing an effective method to easily compare different models.    

```{r initial pdps, warning = FALSE, eval = FALSE}
nano <- nano::nano_pdp(nano = nano,
                       model_no = 1:3, 
                       vars = c("lot_size", "income", "crime_rate", "dist_to_rail_station", "dist_to_unsealed_road", "dist_to_school", "longitude", "PM10", "latitude"),
                       plot = FALSE)
```

```{r echo = FALSE, message = FALSE, warning = FALSE, fig.height = 7, fig.width = 8, fig.cap = "PDPs of initial three models."}
pdp <- rbind(nano$pdp$pdp_1, nano$pdp$pdp_2, nano$pdp$pdp_3)
pdp <- pdp[, .(var_band, mean_response, var)]
pdp[, model_id := rep(c(nano$model$model_1@model_id, nano$model$model_2@model_id, nano$model$model_3@model_id), each = 180)]

pdp_plot <- function(pdp, vars) {
  pdps <- rep(list(NA), length(vars))
  for (var in vars) { 
    if (var %in% c("income", "dist_to_school")) ind = TRUE else ind = FALSE
    pdp <- as.data.frame(pdp)
    dat <- pdp[pdp$var == var,]
    fig <- nano:::quiet(plot_ly(data = dat,
                   x = ~var_band, 
                   y = ~mean_response, 
                   group_by = ~model_id,
                   type = "scatter", 
                   color = ~model_id, 
                   legendgroup = ~model_id,
                   mode = "lines+markers",
                   showlegend = ind) %>% 
                     layout(xaxis = list(title = var,
                                         hoverformat = ",.2s"),
                            yaxis = list(hoverformat = ",.2s"),
                            legend = list(orientation = "h")))
    pdps[[var]] <- fig
  }
  return(pdps)
}

# plots pdps
pdp_plots <- pdp_plot(pdp, unique(pdp$var))
#pdp_plots$lot_size # expected relationship. m1 is bit unstable, m2 better at differentiating 
#pdp_plots$income # expected, m1 and m2 good at diff - perhaps apply monotone constraint to this
#pdp_plots$crime_rate # interesting, m1 unstable
#pdp_plots$dist_to_rail_station # expected, m1 unstable
#pdp_plots$dist_to_unsealed_road # all similar, clear increasing trend - perhaps apply monotone constraint
#pdp_plots$longitude # expected, all are similar
#pdp_plots$latitude # interesting, m1 highly variable, m2 and m3 capture effect of latitude

subplot(pdp_plots$income, pdp_plots$dist_to_unsealed_road, pdp_plots$longitude, 
        nrows = 2,
        titleX = TRUE,
        titleY = TRUE)

## Conclusion
# M1 is very unstable. This is expected since it only has 45 trees, hence high model variance. # M2 shows expected relationships and good at differentiating sale_price by risk factors
# M3 capture most of the expected relationships, however not very strong at differentiating sale_price

```



## Model Building

From the above discussion, it is clear the GBM with the Tweedie distribution (tweedie_power = 1.8) produces the best performance and has a prediction mechanism which is inline with our expectations. In this section, we will consider several different models built using the Tweedie distribution.

### Top 10 Variables

The first model we shall consider is to use only the top 10 important variables from the Tweedie model above. The variable importance can be calculated using the `nano_varimp` function, from which the most important variables can be extracted. 

```{r initial varimp}

nano <- nano::nano_varimp(nano = nano,
                          model_no = 2)
varimp_2 <- data.table::copy(nano$varimp$varimp_2)
```

```{r echo = FALSE, fig.height = 4, fig.cap = "Variable importance of top 10 variables in Tweedie model."}
fig <- plot_ly(x = varimp_2$percentage[1:10]/100, 
               y = factor(varimp_2$variable, levels = rev(varimp_2$variable))[1:10],
               type = 'bar', 
               orientation = 'h') %>% 
  layout(xaxis = list(title = "Importance (%)",
                      hoverformat = ".0%"))
fig
```

```{r top 10, eval = FALSE}

## mode Tweedie 1.8 with only top 10 variables
ignore_vars <- nano$varimp$varimp_2$variable[-(1:10)]
hyper_params$distribution <- "Tweedie"
hyper_params$tweedie_power <- 1.8

nano <- nano::nano_grid(nano                = nano,
                        response            = response,
                        algo                = "gbm", 
                        data                = data,
                        ignore_vars         = ignore_vars,
                        fold_column         = "fold",
                        hyper_params        = hyper_params,
                        strategy            = "Cartesian",
                        stopping_metric     = "deviance",
                        stopping_tolerance  = 0.01,
                        stopping_rounds     = 5,
                        score_tree_interval = 10,
                        grid_description    = "GBM Tweedie 1.8 distribution - top 10 vars.")
```


### Monotone Constraints

The PDPs from the previous section suggest there is a clear increasing relationship between income, dist_to_unsealed_road and sale_price. Such a relationship is expected as well. However, the PDPs is not monotically increasing, but rather zig-zagged. This can be due to the model overfitting and incorrectly capturing random variation, which is a common issue in machine learning models. A method to prevent this is to apply monotone constraints on these variables. This can be done by supplying a list to the `monotone_constraints` argument in the `nano_grid` function.    

```{r monotone constraints, eval = FALSE}

## Tweedie 1.8 with 10 variables plus monotone constraint on income and dist_to_unsealed_road
nano <- nano::nano_grid(nano                 = nano,
                        response             = response,
                        algo                 = "gbm", 
                        data                 = data,
                        ignore_vars          = ignore_vars,
                        fold_column          = "fold",
                        hyper_params         = hyper_params,
                        strategy             = "Cartesian",
                        stopping_metric      = "deviance",
                        stopping_tolerance   = 0.01,
                        stopping_rounds      = 5,
                        score_tree_interval  = 10,
                        monotone_constraints = list(income = 1, dist_to_unsealed_road = 1),
                        grid_description     = "GBM Tweedie 1.8 distribution - top 10 vars with monotone constraint on income and dist_to_unsealed_road.")

```


### Hyper-parameter Tuning

Finally, the last grid we will consider is hyper-parameter tuning on the previous model. Hyper-parameter tuning can be performed by specifying the `hyper_param` argument in the `nano_grid` function. Six hyper-parameters were selected to be tuned. They were:

* min_rows        - minimum number of rows per node.
* max_depth       - maximum depth of each tree
* sample_rate     - percentage of rows sampled for each tree
* col_sample_rate - percentage of columns sampled for each split
* col_sample_rate_per_tree - percentage of columns sampled for each tree
* tweedie_power   - variance power of Tweedie distribution for loss function

Hyper-parameter tunin is important because choosing inappropriate values can result in over-fitting or under-fitting of the model. Hence it is important to consider several different combinations of the above hyper-parameters before selecting the final model (James et al. 2013).

The "learning_rate" hyper-parameter was chosen not to be included in the tuning since as a conservatively small value was already chosen and increasing its value will not improve the model fit. Further, the "ntrees" hyper-parameter was chosen not to be included in the tuning since the model was already fitted with an excess number of trees and early stopping was implemented to avoid overfitting of the model.


```{r hyper-parameter tuning, eval = FALSE}

# hyper-parameter tuning of max_depth in selected model
hyper_params_tune <- list(ntrees                   = 500,
                          max_depth                = seq(3, 9, 2),
                          min_rows                 = seq(10, 40, 10),
                          learn_rate               = 0.01,
                          distribution             = "Tweedie",
                          sample_rate              = seq(0.5, 0.8, 0.1),
                          col_sample_rate          = seq(0.5, 0.8, 0.1),
                          col_sample_rate_per_tree = 0.8, 
                          min_split_improvement    = 1e-05,
                          tweedie_power            = seq(1.8, 1.95, 0.05))

nano <- nano::nano_grid(nano                 = nano,
                        response             = response,
                        algo                 = "gbm", 
                        data                 = data,
                        ignore_vars          = ignore_vars,
                        fold_column          = "fold",
                        hyper_params         = hyper_params_tune,
                        strategy             = "RandomDiscrete",
                        max_models           = 50,
                        max_runtime_secs     = 10 * 60,
                        stopping_metric      = "deviance",
                        stopping_tolerance   = 0.01,
                        stopping_rounds      = 5,
                        score_tree_interval  = 10,
                        monotone_constraints = list(income = 1, dist_to_unsealed_road = 1),
                        grid_description     = "GBM Tweedie 1.8 distribution - top 10 vars with monotone constraints and hyper-parameter tuning.")


```



## Model Checking and Selection {#check}

From the models built above, we shall compare four models (the top models in grids 4 and 5 (M4 and M5 respectively), and the top two models in grid 6 (M6a and M6b respectively)) more closely in detail to select our final model. We are not able to immediately use the `nano` helper functions to compare these four models since it is only able to compare one model per slot (hence it cannot compare models M6a and M6b). However, instead we can use the `grid_to_nano` function to create a new `nano` object where each of the desired models are given a separate slot. Then we can compare the four models using our usual methods.

```{r new nano, warning = FALSE, message = FALSE}

# create new nano object with separate slots for the four required models
nano_fin <- nano:::quiet(nano::grid_to_nano(nano        = nano,
                                            grids_no    = 4:6,
                                            n_top_model = c(1, 1, 2)))
# cv metrics
nano::nano_metrics(nano      = nano_fin,
                   model_no  = 1:4, 
                   data_type = "cv")

```

From the above table, M6a performs the best across all metrics, however the three other models perform very similarly to M6a. However M4 and M5 have a significantly higher residual deviance compared to M6a and M6b. This is due to M6a and M6b having a higher tweedie_power of 1.95. This suggests Tweedie (1.95) distribution is a better fit to the sale_price distribution.

Figure 4.3 shows the training residuals vs the actual values for each of the four models. All models have very similar residuals, however models M4 and M6b have slightly better residuals while M6a has the worse residuals. In general, the model predictions are relatively robust for properties less than $\$1M$, however there is a clear increasing trend in the residuals for properties greater than $\$1M$. This indicates the models underestimate the sale price for more expensive properties. The residuals suggest for future modelling, it would be more appropriate to model lower value ($< \$1M$) and higher value ($>\$1M$) properties separately.  

```{r echo = FALSE, warning = FALSE, message = FALSE, fig.width = 8, fig.cap = "Training residuals of four models."}

# function to plot residuals 
res_plot <- function(nano, n_model, data_type) {
  res_all <- data.table::data.table()
  for (i in n_model) {
    pred <- nano:::quiet(h2o::h2o.predict(object  = nano$model[[i]],
                             newdata = h2o::as.h2o(nano$data[[i]][data_id == data_type])))
    act <- nano$data[[i]][data_id == data_type][[response]]
    dif <- act - data.table::as.data.table(pred)[["predict"]]
    res <- data.table::data.table(model_id  = nano$model[[i]]@model_id,
                                  actual    = act,
                                  residual  = dif)
    res_all <- rbind(res_all, res)
  }
  
  fig <- plot_ly(data = res_all,
                 x = ~actual,
                 y  = ~residual,
                 group_by = ~model_id,
                 type = "scatter",
                 color = ~model_id,
                 mode = "markers") %>% 
    layout(xaxis = list(hoverformat = ",.2s"),
           yaxis = list(hoverformat = ",.2s"))
  
  return(fig)
}

# training residuals for the four models
residuals <- res_plot(nano_fin, 1:4, "train")  
residuals

```


Figure 4.5 compare the PDPs for each of the four models. The PDPs for lot_size, income dist_to_unsealed_road are very similar and follow the expected trend. Note the PDP of income and dist_to_unsealed_road for M5 is monotonically increasing and is smoother than the PDP for M4 and M6b. The PDP of crime_rate is highly variable for lower values. This could signify the models incorrectly capturing random variation. M5 and M6a is the least variable out of the four models. 
Interestingly, as the crime_rate increases above 2.3, there is a rapid increase in sale_price. This is rather unexpected and in fact may be a spurious relationship. One possible explanation for this is that luxurious properties attract more crimes (e.g. theft), therefore it is higher valued properties that attract higher crime rates rather than higher crime rates increasing the sale price of a property.      


```{r final pdps, warning = FALSE, message = FALSE, fig.cap = "PDPs for final four models."}

nano_fin <- nano:::quiet(nano::nano_pdp(nano     = nano_fin,
                                        model_no = 1:4,
                                        vars     = c("lot_size", "income", "crime_rate", "dist_to_hospital", 
                                                     "dist_to_unsealed_road", "PM10", "longitude", "dist_to_medical"),
                                        plot = FALSE))
```

```{r echo = FALSE, warning = FALSE, message = FALSE, fig.height = 8, fig.width = 9, fig.cap = "PDPs of final four models."}

pdp <- rbind(nano_fin$pdp$pdp_1, nano_fin$pdp$pdp_2, nano_fin$pdp$pdp_3, nano_fin$pdp$pdp_4)
pdp <- pdp[, c(1, 2, 7)]
pdp[, model_id := rep(c(nano_fin$model$model_1@model_id, nano_fin$model$model_2@model_id, nano_fin$model$model_3@model_id, nano_fin$model$model_4@model_id), each = 160)]
pdp_fin_plots <- pdp_plot(pdp, unique(pdp$var))

#pdp_fin_plots$income # m5 is a bit smoother, but m6_15 is also very smooth and differentiates better at extremes
#pdp_fin_plots$lot_size # expected and all very similar
#pdp_fin_plots$crime_rate # very variable at small values, m6_15 is the most stable
#pdp_fin_plots$dist_to_unsealed_road # all very similar and show clear increasing trend 
#pdp_fin_plots$PM10 # all highly invariable and difficult to understand trend. Models 4 and 5 least variable, but perhaps don't capture all information
#pdp_fin_plots$longitude # all very similar
#pdp_fin_plots$dist_to_medical # ^
#pdp_fin_plots$dist_to_hospital # ^

subplot(pdp_fin_plots$income, pdp_fin_plots$lot_size, pdp_fin_plots$crime_rate, 
        pdp_fin_plots$dist_to_unsealed_road,
        nrows = 2,
        titleX = TRUE,
        titleY = TRUE)

```

Conclusion: Select model M6a. Has the best performance across all metrics. It also has slightly more robust residuals and provides a more consistent prediction mechanism than the other models as shown in the PDPs. 



# Model Performance Assessment

## Summary Statistics on Predictions based on Holdout Dataset

The final selected model performance was assessed by predicting on the unseen holdout dataset. Summary of the results are shown in table below.  

```{r test performance, warning = FALSE}

nano::nano_metrics(nano_fin, 3, "holdout")

```


## Analysis of Predictions on Holdout Dataset

First, the distribution of predictions were compared with the actuals shown in figure 5.1. The distributions generally match each other, however the predicted values have greater density in lower values compared to the actual sale prices. This corresponds to the residual plots in figure 4.3 which showed the predicted values underestimates the sale prices for higher valued properties. 


```{r echo = FALSE, message = FALSE, warning = FALSE, fig.height = 3.5, fig.width = 9, fig.cap = "Histogram of predictions vs actuals."}

pred <- nano_predict(nano_fin, 4, "holdout")

fig <- plot_ly(alpha = 0.6)
fig <- fig %>% add_histogram(x = ~pred$actual, name = "Actual")
fig <- fig %>% add_histogram(x = ~pred$predicted, name = "Predicted")
fig <- fig %>% layout(barmode = "overlay",
                      xaxis = list(title = "Sale Price"),
                      yaxis = list(title = "Frequency"),
                      title = "Histogram of Predicted vs Actual")
#htmltools::div(fig, align = "center")
fig

```


## Variable Importance

The below plot ranks the relative importance of each variable in the final model. It appears longitude is the most predictive variable by far, explaining over $55\%$ of the model's predictive power. This is expected since it is generally well-known that property prices vary significantly by region and hence by longitude. E.g. Properties with high longitude would have a higher sale price since these properties are in the city center. This fact is supported by the PDPs shown above. 

Interestingly, sale_date is calculated to be the 6th most predictive variable, despite not showing any relationship in the time series plot produced earlier in the report. This may be due to that the variable important calculation algorithm tends to favour variables with higher number of levels. Since sale_date has 364 unique levels, this may explain why it has been assigned a higher then expected variable importance. 

```{r echo = FALSE, fig.height = 3, fig.width = 7, fig.cap = "Variable importance plot of final selected model. Importance of most significant variable is scaled to 1."}

nano_fin <- nano::nano_varimp(nano_fin, 4, plot = FALSE)
varimp_4 <- data.table::copy(nano_fin$varimp$varimp_4)

fig <- plot_ly(x = varimp_4$percentage[1:10]/100, 
               y = factor(varimp_4$variable, levels = rev(varimp_4$variable))[1:10],
               type = 'bar', 
               orientation = 'h') %>% 
  layout(xaxis = list(title = "Importance (%)",
                      hoverformat = ",.0%"))
fig

```


## Partial Depdency Plots (PDP)

The PDPs for the final model has already been discussed in previous sections. However, we shall give a brief analysis on some of the key PDPs.

* Longitude - A longitude increases, there is initially a small drop in sale prices, and then its begins to sharply increase. This reflects how the most eastern suburbs are the most affulent (e.g. North Sydney) compared to western suburbs and hence will tend to have higher sale prices. 

* Distance to hospital - There is a significant drop in sale price as distance to hospital increases to 5 km, and then it flattens out. The PDP suggest properties within a 1 km radius to hospitals are about $\$40K$ more expensive to other properties further out. 

* Distance to medical - This has a similar relationship to dist_to_hospital with an even more sharper decrease in sale price as distance to medical centre increases. The PDP states properties which are right next to a medical centre are worth $\$60K$ more than properties which are 5 km or more away from a medical center.

* Distance to school (this is produced from earlier models created) - Interestingly, sale prices increase for properties further away from schools before flattening out. The PDPs suggest properties which are at least 3 km away from schools are worth more. 

* Distance to railway station (this is produced from earlier models created) - Similarly to above, interestingly, the PDPs predict that sale prices increase for as properties become further away from railway stations. 

```{r echo = FALSE, warning = FALSE, message = FALSE, fig.height = 4, fig.width = 8, fig.cap = "PDPs of initial saturated Tweedie model. Left: Distance to school. Right: Distance to railway station."}

subplot(pdp_plots$dist_to_school, pdp_plots$dist_to_rail_station,
        nrows = 1,
        shareY = TRUE,
        titleY = TRUE)

```


## Interaction Plots

One of the main advantages of GBM models is it is easily able to capture interactions between variables. Using the `IML` package, it is possible to quantify the level of pairwise (first order) interaction between each variable captured by the GBM model. This is illustrated in figure 5.5. 

These plots are useful for building "white-box" model from a "black-box" model. E.g. using the key interactions identified by figure 5.5, you are able to manually insert these as interaction terms when building a GLM on the same dataset. This approach can often result in comparable results despite GLMs being much simplier and more intepretable compared to GBMs.

```{r eval = FALSE}
nano_fin <- nano::nano_interaction(nano_fin, 4, "longitude")

```

```{r echo = FALSE, fig.height=3, fig.widtht=5, fig.cap = "Plot of pairwise interaction between longitude and all other variables captured by the final mode."}

ice_4 <- readRDS(paste0(path, "/ice_4"))
fig <- plot_ly(x = ice_4$interaction, 
               y = factor(ice_4$feature, levels = ice_4$feature[order(ice_4$interaction)]),
               type = 'bar', 
               orientation = 'h') %>% 
  layout(xaxis = list(title = "Interaction",
                      hoverformat = ".2f"))
fig

```

# Save Nano Object

After all modelling has been completed, the `nano` object can be saved in a chosen directory as shown below. It can then be loaded again by using the `nano_load` function. Importantly, the `nano` object can only be saved in an empty directory, otherwise it will not be loaded correctly when running `nano_load`. After the `nano` has been correctly saved, to terminate the connection to the h2o cluster, run `nano_shutdown`. Running this function will remove all grids and models form the cluster, hence only run this once the `nano` object has been saved. 

```{r save nano object, eval = FALSE}
nano::nano_save(nano_fin, path)
nano::nano_shutdown()
```






